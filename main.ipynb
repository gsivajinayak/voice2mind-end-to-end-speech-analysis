{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4llWn8whKWEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4c57fe91-c45f-464d-aa3c-29aebdf8f0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 - Install dependencies\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!pip install gradio -q\n",
        "!pip install transformers accelerate sentencepiece gTTS -q\n",
        "!pip install tensorflow -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 - Imports & device\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "import whisper\n",
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    DistilBertTokenizerFast,\n",
        "    TFDistilBertForSequenceClassification,\n",
        "    MBartForConditionalGeneration,\n",
        "    MBart50TokenizerFast\n",
        ")\n",
        "\n",
        "# Device for whisper and pipelines\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device set to use {DEVICE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_uiLTzP9QdvB",
        "outputId": "c410ab89-8865-426d-90ad-533285ce0a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 - Whisper, English grammar corrector, and simple translation pipelines\n",
        "# Whisper STT\n",
        "whisper_model = whisper.load_model(\"small\", device=DEVICE)\n",
        "\n",
        "# English grammar corrector (T5 fine-tuned)\n",
        "en_corrector = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"vennify/t5-base-grammar-correction\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Helsinki translation pipelines for quick Hindi<->English translation\n",
        "hi_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-hi-en\")\n",
        "en_to_hi = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c3lHgN_uQw4V",
        "outputId": "f4137977-8fdc-4875-e981-558444c6498a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 - mBART (alternative Hindi->English translation, used in pipeline)\n",
        "mbart_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(mbart_model_name)\n",
        "mbart_model = MBartForConditionalGeneration.from_pretrained(mbart_model_name)\n",
        "\n",
        "def translate_hi_to_en(text: str) -> str:\n",
        "    mbart_tokenizer.src_lang = \"hi_IN\"\n",
        "    inputs = mbart_tokenizer(text, return_tensors=\"pt\")\n",
        "    generated_tokens = mbart_model.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=mbart_tokenizer.lang_code_to_id[\"en_XX\"]\n",
        "    )\n",
        "    return mbart_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MlFOHyltRAH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A - Inspect saved model folder\n",
        "import os\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/distilbert-mental-health-model\"\n",
        "print(\"SAVE_DIR:\", SAVE_DIR)\n",
        "print(\"Exists:\", os.path.exists(SAVE_DIR))\n",
        "print(\"\\nFiles in SAVE_DIR (recursive):\")\n",
        "for root, dirs, files in os.walk(SAVE_DIR):\n",
        "    level = root.replace(SAVE_DIR, \"\").count(os.sep)\n",
        "    indent = \" \" * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    for f in files:\n",
        "        print(f\"{indent}  - {f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AVU1C3rpRQnr",
        "outputId": "698843fd-e695-482f-8d34-fe1f01aa3dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAVE_DIR: /content/drive/MyDrive/distilbert-mental-health-model\n",
            "Exists: True\n",
            "\n",
            "Files in SAVE_DIR (recursive):\n",
            "distilbert-mental-health-model/\n",
            "  distilbert-mental-health-model/\n",
            "    - config.json\n",
            "    - label_encoder.pkl\n",
            "    - special_tokens_map.json\n",
            "    - tf_model.h5\n",
            "    - tokenizer.json\n",
            "    - tokenizer_config.json\n",
            "    - vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
        "import pickle, os, tensorflow as tf\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/distilbert-mental-health-model/distilbert-mental-health-model\"\n",
        "\n",
        "# If vocab.txt missing, copy from base tokenizer\n",
        "vocab_path = os.path.join(SAVE_DIR, \"vocab.txt\")\n",
        "if not os.path.exists(vocab_path):\n",
        "    print(\"Adding missing vocab.txt ...\")\n",
        "    base_tok = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "    base_tok.save_pretrained(SAVE_DIR)\n",
        "    print(\"✅ vocab.txt and other tokenizer files added to:\", SAVE_DIR)\n",
        "else:\n",
        "    print(\"vocab.txt already exists — skipping creation.\")\n",
        "\n",
        "# Reload tokenizer & model to confirm everything works\n",
        "sentiment_tokenizer = DistilBertTokenizerFast.from_pretrained(SAVE_DIR)\n",
        "sentiment_model = TFDistilBertForSequenceClassification.from_pretrained(SAVE_DIR)\n",
        "\n",
        "with open(os.path.join(SAVE_DIR, \"label_encoder.pkl\"), \"rb\") as f:\n",
        "    sentiment_label_encoder = pickle.load(f)\n",
        "\n",
        "print(\"✅ Tokenizer + model + label_encoder loaded successfully!\")\n",
        "print(\"Classes:\", list(sentiment_label_encoder.classes_))\n",
        "\n",
        "# Quick check\n",
        "text = \"I feel hopeless and empty every day.\"\n",
        "inputs = sentiment_tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
        "logits = sentiment_model(inputs).logits\n",
        "probs = tf.nn.softmax(logits, axis=1)\n",
        "pred = tf.argmax(probs, axis=1).numpy()[0]\n",
        "conf = float(tf.reduce_max(probs).numpy())\n",
        "print(f\"Example Prediction: {sentiment_label_encoder.classes_[pred]} ({conf:.2%})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ADs2jWyTRV8s",
        "outputId": "cf890d61-60fe-444e-cfef-622bc7072804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab.txt already exists — skipping creation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
            "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/distilbert-mental-health-model/distilbert-mental-health-model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tokenizer + model + label_encoder loaded successfully!\n",
            "Classes: [np.str_('Anxiety'), np.str_('Bipolar'), np.str_('Depression'), np.str_('Normal'), np.str_('Personality disorder'), np.str_('Stress'), np.str_('Suicidal')]\n",
            "Example Prediction: Depression (65.62%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 - Load saved DistilBERT sentiment/mental-health model\n",
        "SAVE_DIR = \"/content/drive/MyDrive/distilbert-mental-health-model/distilbert-mental-health-model\"\n",
        "\n",
        "sentiment_model = TFDistilBertForSequenceClassification.from_pretrained(SAVE_DIR)\n",
        "sentiment_tokenizer = DistilBertTokenizerFast.from_pretrained(SAVE_DIR)\n",
        "\n",
        "with open(os.path.join(SAVE_DIR, \"label_encoder.pkl\"), \"rb\") as f:\n",
        "    sentiment_label_encoder = pickle.load(f)\n",
        "\n",
        "print(\"Sentiment model + tokenizer + label encoder loaded successfully\")\n",
        "print(\"Available classes:\", list(sentiment_label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZfyVCPlZRjZ5",
        "outputId": "cb8cda84-05f6-4ff3-a426-5dd35f83842d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at /content/drive/MyDrive/distilbert-mental-health-model/distilbert-mental-health-model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/distilbert-mental-health-model/distilbert-mental-health-model and are newly initialized: ['dropout_39']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment model + tokenizer + label encoder loaded successfully\n",
            "Available classes: [np.str_('Anxiety'), np.str_('Bipolar'), np.str_('Depression'), np.str_('Normal'), np.str_('Personality disorder'), np.str_('Stress'), np.str_('Suicidal')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 - Utility functions\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def predict_sentiment(text: str):\n",
        "    inputs = sentiment_tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
        "    logits = sentiment_model(inputs).logits\n",
        "    probs = tf.nn.softmax(logits, axis=1)\n",
        "    pred_class = tf.argmax(probs, axis=1).numpy()[0]\n",
        "    confidence = float(probs[0][pred_class].numpy())\n",
        "    predicted_status = sentiment_label_encoder.classes_[pred_class]\n",
        "    return predicted_status, confidence\n",
        "\n",
        "# optional: contraction normalizer used elsewhere in older notebook (kept for compatibility)\n",
        "def expand_contractions(text: str) -> str:\n",
        "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
        "    text_l = text.lower()\n",
        "    CONTRACTION_MAP = {\n",
        "        \"i'm\": \"I am\",\n",
        "        \"i've\": \"I have\",\n",
        "        \"i'd\": \"I would\",\n",
        "        \"i'll\": \"I will\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"n't\": \" not\",\n",
        "        \"'re\": \" are\",\n",
        "        \"'s\": \" is\",\n",
        "        \"'ve\": \" have\",\n",
        "        \"'d\": \" would\",\n",
        "        \"'ll\": \" will\"\n",
        "    }\n",
        "    for contraction, expansion in CONTRACTION_MAP.items():\n",
        "        text_l = re.sub(r\"\\b{}\\b\".format(re.escape(contraction)), expansion, text_l)\n",
        "    # Keep original capitalization for final text (simple approach)\n",
        "    return text_l\n"
      ],
      "metadata": {
        "id": "lbkUopYMZI8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 - English/Hindi audio processing pipeline\n",
        "def english_pipeline(audio_filepath):\n",
        "    result = whisper_model.transcribe(audio_filepath)\n",
        "    text = clean_text(result[\"text\"])\n",
        "    detected_lang = result[\"language\"]\n",
        "\n",
        "    if detected_lang == \"en\":\n",
        "        corrected = en_corrector(text, max_length=512)[0]['generated_text']\n",
        "        corrected = clean_text(corrected)\n",
        "        status, conf = predict_sentiment(corrected)\n",
        "\n",
        "        tts = gTTS(corrected, lang=\"en\")\n",
        "        filename = f\"/tmp/{uuid.uuid4()}.mp3\"\n",
        "        tts.save(filename)\n",
        "\n",
        "        return f\"Detected: English\\nCorrected: {corrected}\\nSentiment: {status} ({conf:.1%})\", filename\n",
        "\n",
        "    elif detected_lang == \"hi\":\n",
        "        # Use mBART for better hi->en as in original pipeline\n",
        "        en_text = translate_hi_to_en(text)\n",
        "        en_text = clean_text(en_text)\n",
        "\n",
        "        status, conf = predict_sentiment(en_text)\n",
        "\n",
        "        hi_text = en_to_hi(en_text)[0]['translation_text']\n",
        "        hi_text = clean_text(hi_text)\n",
        "\n",
        "        tts = gTTS(hi_text, lang=\"hi\")\n",
        "        filename = f\"/tmp/{uuid.uuid4()}.mp3\"\n",
        "        tts.save(filename)\n",
        "\n",
        "        return (\n",
        "            f\"Detected: Hindi\\nHindi: {text}\\nEnglish: {en_text}\\nSentiment: {status} ({conf:.1%})\",\n",
        "            filename\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        return f\"Unsupported language detected: {detected_lang}\", None\n"
      ],
      "metadata": {
        "id": "Gu8GN7vxZdoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 - English text input processing\n",
        "def english_text_input(text):\n",
        "    text = clean_text(text)\n",
        "    corrected = en_corrector(text, max_length=512)[0]['generated_text']\n",
        "    corrected = clean_text(corrected)\n",
        "\n",
        "    status, conf = predict_sentiment(corrected)\n",
        "\n",
        "    tts = gTTS(corrected, lang=\"en\")\n",
        "    filename = f\"/tmp/{uuid.uuid4()}.mp3\"\n",
        "    tts.save(filename)\n",
        "\n",
        "    return f\"Corrected: {corrected}\\nSentiment: {status} ({conf:.1%})\", filename\n"
      ],
      "metadata": {
        "id": "FY1wc3z3ZgZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 - Hindi text input processing\n",
        "def hindi_text_input(text):\n",
        "    text = clean_text(text)\n",
        "    en_text = hi_to_en(text)[0]['translation_text']\n",
        "    en_text = clean_text(en_text)\n",
        "\n",
        "    status, conf = predict_sentiment(en_text)\n",
        "\n",
        "    hi_text = en_to_hi(en_text)[0]['translation_text']\n",
        "    hi_text = clean_text(hi_text)\n",
        "\n",
        "    tts = gTTS(hi_text, lang=\"hi\")\n",
        "    filename = f\"/tmp/{uuid.uuid4()}.mp3\"\n",
        "    tts.save(filename)\n",
        "\n",
        "    return (\n",
        "        f\"Hindi Input: {text}\\nEnglish: {en_text}\\nSentiment: {status} ({conf:.1%})\",\n",
        "        filename\n",
        "    )\n"
      ],
      "metadata": {
        "id": "br4OV4utZjNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 - Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Speech/Text Grammar + Sentiment (English & Hindi)\")\n",
        "\n",
        "    with gr.Tab(\"English/Hindi Audio\"):\n",
        "        inp_audio = gr.Audio(sources=[\"microphone\", \"upload\"], type=\"filepath\")\n",
        "        out_text = gr.Textbox(label=\"Corrected / Translated Text + Sentiment\")\n",
        "        out_audio = gr.Audio(label=\"Speech Output\", type=\"filepath\")\n",
        "        btn_audio = gr.Button(\"Process\")\n",
        "        btn_audio.click(fn=english_pipeline, inputs=inp_audio, outputs=[out_text, out_audio])\n",
        "\n",
        "    with gr.Tab(\"English Text\"):\n",
        "        inp_text_en = gr.Textbox(label=\"Enter English text\")\n",
        "        out_text_en = gr.Textbox(label=\"Corrected English + Sentiment\")\n",
        "        out_audio_en = gr.Audio(label=\"Corrected Speech\", type=\"filepath\")\n",
        "        btn_text_en = gr.Button(\"Process\")\n",
        "        btn_text_en.click(fn=english_text_input, inputs=inp_text_en, outputs=[out_text_en, out_audio_en])\n",
        "\n",
        "    with gr.Tab(\"Hindi Text\"):\n",
        "        inp_text_hi = gr.Textbox(label=\"Enter Hindi text\")\n",
        "        out_text_hi = gr.Textbox(label=\"English Translation + Sentiment\")\n",
        "        out_audio_hi = gr.Audio(label=\"Hindi Speech Output\", type=\"filepath\")\n",
        "        btn_text_hi = gr.Button(\"Process\")\n",
        "        btn_text_hi.click(fn=hindi_text_input, inputs=inp_text_hi, outputs=[out_text_hi, out_audio_hi])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "InVCoZHeZlXt",
        "outputId": "34cb4658-22bb-4045-f24b-2908607d8940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f837171f7b7d2a9ddf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f837171f7b7d2a9ddf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zF_jMKWZs0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}